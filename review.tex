\documentclass{article}
\title{Automomous Control of Aircraft for Communications and
  Electronic Warfare: Insights and Promises of Recent Literature}
\author{Sean Carver, Ph.D. at Data Machines Corporation}
\begin{document}
\maketitle
\abstract{We pose an unsolved problem in autonomous control of
  aircraft for communications and jamming (electronic warfare) and
  review the literature relevant to this problem. Some work offers
  approximately optimal solutions to related problems in different
  domains---promising applicability to the important scenarios
  considered here.  Other work covers methods that we may find useful
  in extending these relevant solutions.
  
  The problem we address lies within the fields of adversarial
  Multi-Agent Reinforcement Learning (MARL) and active sensing.  In
  our problem, two opposing factions (labeled ``blue'' and ``red'')
  compete to win a zero-sum/purely adversarial game.  The blue side
  tries to maintain communication links between ground-based assets
  with a fleet of ``comms;'' whereas the red side tries to jam this
  network with a fleet of ``jammers.''  An Unmanned Aerial Vehicle
  (UAV) becomes a comm or a jammer when fitted for one of these
  purposes.

  Each faction lacks knowledge and access to the state of the opposing
  side, but benefits from inferring this state probabilisticly through
  positioning its fleet for best sensory performance and localization
  (active sensing).  This maneuvering should take into account the
  real possibility of any UAV getting shot down by its adversary's
  appropriately positioned ground troops.  That said, the objective
  of each side should remain foremost.  The blue side aims to
  simultaneously keep all units in communication while the red side
  aims to simultaneously jam this communication.  Despite best
  efforts, different units/UAVs can fall in and out of communication
  with their respective headquarters, making each of the blue and red
  factions a multi-agent collection, fully cooperating among itself,
  despite different information, to fight its adversary having
  opposing goals.  Our contribution poses this problem while pointing
  to literature for possible ideas for moving the field forward
  towards a successful implementation for the full adversarial problem
  in real-world combat.}

\section{Introduction}

If unfortunate circumstances compel our leaders to order our armed
forces to take a city from an adversary, the command headquarters on
the ground would benefit from constant two-way communication with all
its other units during the conflict.

In the fog that accompanies such struggles, our forces cannot rely on
our enemy's network of cell towers to keep in touch.  Instead, two way
radios, linked by a network of ``comms'' (UAVs for communication) will
hopefully allow our friendlies to stay connected.

While vastly better than cell phones, such a network has it own set of
challenges.  Indeed, our adversaries clearly prefer to keep us out of
communication.  To pursue this preference, they may send up jammers
(UAVs for blocking communication).  Thus begins a delicate dance of
each side positioning its fleet to best find the other's birds and in
so doing best keep or block communications.

We study the question of how each side can control its fleet by
autonomously ordering and carrying out flight and communications-
electronics operation instructions (CEOI) to optimally achieve its
objectives.  We are interested in the strategies for both sides,
because to defeat our enemy, we must understand the intelligent
countermeasures they may take. Moreover, in a real war, our side---as
well as theirs---may choose to fly both comms and jammers, requiring
strategies for both roles.

Recent literature has tackled the problem of near-optimal search and
rescue \cite{hoffmann2009mobile} and other related search and
localization paradigms \cite{ryan2010particle, tisdale2008multiple,
  gustafsson2002particle}.  Other work has considered different
applications requiring similar tools---notably cyber-security
\cite{nicholson2007information} and precision farming
\cite{testi2020reinforcement}.  Search and rescue, for example,
clearly relates to the problem at hand because, as with rescue, each
side in our conflict benefits from successfully inferring the
positions of targets on the other side.  But electronic warfare
differs from search and rescue.  People being rescued presumably want
to be found and will presumably cooperate with this effort.  In
electronic warfare, on the other hand, targets aim to conceal their
true locations from their adversaries. As a result, while search and
rescue can succeed with a purely active sensing and optimal control
solution, in our scenario, we need to learn to counter an opponent's
strategy.  To this end, we propose to apply artificial intelligence:
specifically, adversarial multi-agent reinforcement learning. This
paper reviews the literature relevant to this approach to victory.

\section{Optimal and sub-optimal filtering}
The filtering problem takes measurements of a stochastic
system---possibly transformed measurements and possibly with
noise---and produces estimates of the state of the system.  This
section reviews classical work on this problem, citing textbooks
instead of recent papers.

Readers will find the optimal solution to this problem in the first
pages of many textbooks on nonlinear filtering \cite{ristic2003beyond,
  crisan2011oxford, smith2013sequential, chopin2020introduction}.  The
solution implements a recursion consisting of alternating applications
of the Chapman-Kolmogorov Equation and Bayes Rule.

Unfortunately, solving each of these equations demands an integration
remaining provably intractable in most cases---indeed in all but two
cases that researches have already identified.  In all other cases, a
researcher must settle for an approximation---a sub-optimal (but
hopefully still \emph{approximately} optimal) filter.  Readers will
find that the rest of the nonlinear filtering textbooks (the rest
beyond the first few pages devoted to the optimal exposition) develop
these sub-optimal approximations.

First, we list the two truly optimal solutions to the filtering
equation as (1) the Kalman filter, and (2) the finite hidden Markov
filter.  The Kalman filter uses a linear model of the process, a
quadratic objective function measuring optimality, and Gaussian noise
corruption (LQG problem).  The Gaussian assumption must hold for both
the process noise and the measurement noise.  The LQG problem divides
into the linear quadratic estimator (LQE) problem for the optimal
state estimates and the linear quadratic regulator (LQR) for the
optimal control of such a system.  As we will see below, the solutions
of these problems decouple in a surprising and useful way (the
separation principle, see the last section below) but unfortunately,
this decoupling makes use of the specific assumptions we have imposed
here.

Likewise, a finite hidden Markov filter uses a finite-state model of
the process. These restrictions unacceptably constrain usable models
for our application area, and therefore we will focus on approximately
optimal alternatives.

Several specific approximations merit mention.  An extended Kalman
filter linearizes the state space around each sample point allowing
the calculations behind the Kalman filter to proceed approximately,
even when the assumptions for using a Kalman filter do not hold
exactly. The approximation works well when the optimal probability
distributions for state remain close to Gaussian.  If they do not
remain approximately Gaussian, the Extended Kalman Filter can perform
poorly, leading to poor state estimates and impoverished inference.

A second approximation, a grid-based filter, approximates the state
space with a finite grid of points allowing the calculations behind a
hidden (finite) Markov filter to proceed, even for infinite state
spaces.  A grid-based filter works well for the lowest dimensional
state spaces, but becomes computationally intractable when the
dimensions become even slightly higher.  In preliminary investigations
the electronic warfare problem that motivated this review [Carver,
research paper in preparation], one and two targets worked well (each
adding two dimensions, longitude and latitude, to the state), whereas
three simultaneous targets remained expensive beyond reach.  In this
work, we aimed to find jammers (``targets'') without bearing
information from observing successful or unsuccessful radio
connections to friendlies.

The field calls the last class of filtering approximations that
deserves our attention "particle filters."  In short, the idea
approximates evolving distributions with a finite swarm of Monte Carlo
sample points called particles.  These methods possess great
generality and flexibility, but many researchers find particle methods
more difficult to understand, and to successfully implement, than
their simpler and more straightforward cousins.  Note that there exist
many different ways to implement particle filters, each with its own
benefits and limitations.  We will discuss these methods further in
the next section, as several papers concerning Active Sensing use
particle filters.

\section{Active sensing}

Active sensing solves a control problem, and as such has considerable
overlap with reinforcement learning.  Both use observations to select
actions with some notion of how to make that selection.  Whereas
reinforcement learning tries to optimize cumulative reward, many
implementations of active sensing choose its action (they call it
control) to maximize information or, equivalently, minimize entropy,
in the distributions for the estimated quantities.  For example, in
active sensing for search and rescue, we want to control the sensors
(choose the action) to best reduce the uncertainty in the target
locations.

The electronic warfare application that we consider does not have this
form.  We aim not just to locate the other side's birds, but also we
aim to keep or block communication between blue units.  This
additional objective lands this problem squarely in the purvue of
reinforcement learning.  That said, the objective benefits from
knowing, even with uncertainty, the locations of other side's UAVs.
Moreover, like with active sensing, each side can move its own fleet
to localize the other's but, with electronic warfare, it becomes a
means to the end of meeeting its other objectives.  Therefore, we look
to the literature on active sensing for inspriation.

\section{Reinforcement learning and its extensions}

This section spans several disciplines, including reinforcement
learning, deep reinforcement learning, distributional reinforcement
learning, Bayesian reinforcement learning, and multi-agent
reinforcement learning (see citations below).

Let us start by defining the terms above.  Reinforcement learning (RL)
\cite{sutton2018reinforcement, kaelbling1996reinforcement} extends
machine learning to sequential problems where an agent or agents learn
to interact with an environment to maximize cumulative reward.

RL shares a sizable overlap with control theory in engineering,
particularly adaptive control \cite{aastrom2013adaptive,
  khan2012reinforcement}.  The terminology in engineering differs from
the terminology in machine learning, but the terms map to each other.
The ``controller'' (agent) interacts with ``the plant'' (the
environment) by selecting a ``control'' (an action) that ``minimizes
cost'' (maximizes reward).  Engineers typically deal with continuous
systems (ie robotics), whereas many, but not all, reinforcement
solutions have finite action and state spaces.  Adaptive control,
moreover, aims more to maintain control when the plant changes, rather
than to learn to control the plant \emph{de novo}.  Despite cosmetic
differences, clearly if both systems solve the same problem, and to
the extent that they solve it optimally, with the same optimality
criteria, the solutions must coincide.  But the approximations to
optimality made in each discipline may differ, of course.  We expect
to find the same pattern when we fold active sensing in with
reinforcement learning.

Deep RL \cite{li2017deep} uses neural networks to represent the
functions learned by the agent(s).  Classically, RL implementations
deal with inevitable uncertainty in represented quantities by
maintaining best point estimates for these quantities.  Distributional
RL \cite{osband2018randomized} departs from this tradition by
maintaining full probability distributions for the uncertain
quantities. If the actor(s) also perform Bayesian inference on these
distributions (as they generally do), the actor implements Bayesian RL
\cite{ghavamzadeh2016bayesian}.  Finally multi-agent RL
\cite{bucsoniu2010multi} extends RL to environments that include other
interacting agents cooperating or competing for reward.

A lot of good work exists in classical RL and classical deep RL, but
we will not review any papers in these fields.  Instead, we would like
to direct the reader's attention to distributional RL, its slightly
smaller subset, Bayesian RL.  Consider that the target localization and
active sensing literature of interest represent uncertain quantities
with distributions, just like distributional RL.

While an RL algorithm could derive point state estimates from
distributional state information to decide on an action, such an
approach seems wasteful.  Achieving an optimal solution to the
electronic warfare problem stands as a worthy ambition for a machine
learning engineer.  That said, to not using the full distribution
returned by filtering amounts to giving up on this ambition, or so it seems.

However, the mathematics can sometimes work out so that throwing away
the distribution for a point estimate succeeds as the optimal solution
\cite{aastrom2012introduction}.  For example, to control a linear
regulator, where a quadratic cost function determines optimality,
corrupted by Gaussian noise, (the LQR problem) the optimal solution
uses a Kalman filter to produce optimal state estimates, then applies
the optimal control for a determinisitic regulator with known state
equal to those optimal state estimates.  In this case, throwing away
the distribution allowed a simpler but just as good---indeed
optimal---solution.

A system satisfies the so-called \emph{separation principle} if such a
statement holds for the system.  But systems do not always satisfy the
separation principle---those that do stand as the exceptions.  If an
analyst can get away with invoking the separation principle, the
calculations greatly simplify.  Indeed, for the LQR problem, the
solution exists in closed form.

We do not know how well the separation principle applies in our
problem and we do not know how computationally expensive it will
become to use purely distributional methods.  The approach we suggest
applies distributional RL (or more precisely Bayesian RL) to simple
but related problems first then push the envelope both in terms of the
complexity of the scenario, and in terms of the approximations used,
such as the separation principle assuming it applies approximately.

In the LQR problem we have a precise notion of optimality (the
quadratic cost function) and a proof that the solution presented
optimizes this conditon.  In our scenario we have neither.  We can
say, however, that we will apply principles, such as Bayes Rule, that
researchers in other contexts have shown lead to provably optimal
solutions.  We can say that ``optimal'' principles underlie our
methods, rather than that they achieve optimality.  Approaching the
problem in this way, we hope that our solutions will stand as very
good, indeed good enough, and perhaps the very best.

Finally, a multi-agent enviroment such as the one needed for
electronic warfare consists of actors other than self making decisions
based on other hidden information.  This becomes an additional wrinkle
which we must deal with.  Reinforcement learning algorithms divide
into \emph{model-based} algorithms and \emph{model-free} algorithms.
The two methods differ in that a model-based algorithm maintains a
model of the environment and uses this model to select its actions.

Model-based reinforcement learning would fail catastrophically with
our scenario in the real world.  The model in the model-based
algorithm would have to include models of all other agents, including
those of our adversary.  But we must expect that our adversaries will
exploit any modeling assumptions we make---and posing a model of our
adversaries requires such assumptions.

The model-free alternative presents itself as an option.  A recent
comprehensive survey of BRL \cite{ghavamzadeh2016bayesian} discusses
and develops only two such classes of algorithms in that context:
\emph{Bayesian Policy Gradient Algorithms} \cite{engel2007bayesian,
  ghavamzadeh2016abayesian} and \emph{Bayesian Actor-Critic
  Algorithms} \cite{ghavamzadeh2007bayesian,
  ghavamzadeh2016abayesian}.  These observations together with our
preference for BRL, narrow considerably the initially wide field of RL
algorithms of interest.  That said, it seems that just one set of
authors has produced most of the work on model-free Bayesian RL, with
the rest of the community slow to adopt their ideas.  This observation
gives us cause for concern, but no cause to dismiss their work without
further investigation.  The path forward appears clear: test BRL with
simple models, then push the envelope on the complexity of the models,
finally examine the performance/computational cost trade offs with
less expensive suboptimal techniques.

% \nocite{*}
\bibliographystyle{ieeetr}
% \bibliographystyle{unsrt}
\bibliography{marl}

\end{document}
