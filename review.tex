\documentclass{article} \title{Automomous Control of Aircraft for
  Communications and Electronic Warfare: The Promises of Recent
  Artifical Intelligence Literature}
\author{Sean Carver, Ph.D. at Data Machines Corporation}
\begin{document}
\maketitle
\abstract{We pose an unsolved problem in autonomous control of
  aircraft for communications and jamming (electronic warfare) and
  review the literature relevant to this problem. Some work offers
  approximately optimal solutions to related problems in different
  domains---promising applicability to the important scenarios
  considered here.  Other work covers methods that we may find useful
  in extending these relevant solutions.
  
  The problem we address lies within the fields of adversarial
  Multi-Agent Reinforcement Learning (MARL) and active sensing.  In
  our problem, two opposing factions (labeled ``blue'' and ``red'')
  compete to win a zero-sum/purely adversarial game.  The blue side
  tries to maintain communication links between ground-based assets
  with a fleet of ``comms;'' whereas the red side tries to jam this
  network with a fleet of ``jammers.''  An Unmanned Aerial Vehicle
  (UAV) becomes a comm or a jammer when fitted for one of these
  purposes.

  Each faction lacks knowledge and access to the state of the opposing
  side, but benefits from inferring this state probabilisticly through
  positioning its fleet for best sensory performance and localization
  (active sensing).  This maneuvering should take into account the
  real possibility of any UAV getting shot down by its adversary's
  ground troops if appropriately positioned.  That said, the objective
  of each side should remain foremost.  The blue side aims to
  simultaneously keep all units in communication while the red side
  aims to simultaneously jam this communication.  Despite best
  efforts, different units/UAVs can fall in and out of communication
  with their respective headquarters, making each of the blue and red
  factions a multi-agent collection, fully cooperating among itself,
  despite different information, to fight its adversary having
  opposing goals.  Our contribution poses this problem while pointing
  to literature for possible ideas for moving the field forward
  towards a successful implementation for the full adversarial problem
  in real-world combat.}

\section{Introduction}

If unfortunate circumstances compel our leaders to order our armed
forces to take a city from an adversary, the command headquarters on
the ground would benefit from constant two-way communication with all
its other units during the conflict.

In the fog that accompanies such struggles, our forces cannot rely on
our enemy's network of cell towers to keep in touch.  Instead, two way
radios, linked by a network of ``comms'' (UAVs for communication) will
hopefully allow our friendlies to stay connected.

While vastly better than cell phones, such a network has it own set of
challenges.  Indeed, our adversaries clearly prefer to keep us out of
communication.  To pursue this preference, they may send up jammers
(UAVs for blocking communication).  Thus begins a delicate dance of
each side positioning its fleet to best find the other's birds and in
so doing best keep or block communications.

We study the question of how each side can control its fleet by
autonomously ordering and carrying out flight and communications-
electronics operation instructions (CEOI) to optimally achieve its
objectives.  We are interested in the strategies for both sides,
because to defeat our enemy, we must understand the intelligent
countermeasures they may take. Moreover, in a real war, our side---as
well as theirs---may choose to fly both comms and jammers, requiring
strategies for both roles.

Recent literature has tackled the problem of near-optimal search and
rescue \cite{hoffmann2009mobile} and other related search and
localization paradigms \cite{ryan2010particle, tisdale2008multiple,
  gustafsson2002particle}.  Other work has considered different
applications requiring similar tools---notably cyber-security
\cite{nicholson2007information} and precision farming
\cite{testi2020reinforcement}.  Search and rescue, for example,
clearly relates to the problem at hand because, as with rescue, each
side in our conflict clearly benefits from successfully inferring the
positions of targets on the other side.  But there is a difference
between search and rescue and electronic warfare.  People being
rescued presumably want to be found and will presumably cooperate with
this effort.  In electronic warfare, on the other hand, targets aim to
conceal their true locations from their adversaries. As a result,
while search and rescue can succeed with a purely active sensing and
optimal control solution, in our scenario, we need to learn to counter
an opponent's strategy.  To this end, we propose to apply artificial
intelligence: specifically, adversarial multi-agent reinforcement
learning. This paper reviews the literature relevant to this approach
to victory.

\section{Optimal and sub-optimal filtering}
The filtering problem takes measurements of a stochastic
system---possibly transformed measurements and possibly with
noise---and produces estimates of the state of the system.

Readers will find the optimal solution to this problem in the first
pages of many textbooks on nonlinear filtering \cite{ristic2003beyond,
  crisan2011oxford, smith2013sequential, chopin2020introduction}.  The
solution implements a recursion consisting of alternating applications
of the Chapman-Kolmogorov Equation and Bayes Rule.

Unfortunately, solving each of these equations demands an integration
remaining provably intractable in most cases---indeed in all but two
cases that researches have already identified.  In all other cases, a
researcher must settle for an approximation---a sub-optimal (but
hopefully still \emph{approximately} optimal) filter.  Readers will
find that the rest of the nonlinear filtering textbooks (the rest
beyond the first few pages devoted to the optimal exposition) develop
these sub-optimal approximations.  All of the methods discussed in
this section can be found in many such textbooks on filtering, so we
will not give historical pointers to the literature.

First, we list the two truly optimal solutions to the filtering
equation as (1) the Kalman filter, and (2) the finite hidden Markov
filter.  The Kalman filter uses a linear model of the process, a
quadratic objective function measuring optimality, and Gaussian noise
corruption (LQG problem) (Gaussian in both the process noise and the
measurement noise).  On the other hand, a finite hidden Markov filter
uses a finite-state model of the process. These restrictions
unacceptably constrain usable models for our application area, and
therefore we will focus on approximately optimal alternatives.

Several specific approximations merit mention.  An extended Kalman
filter linearizes the state space around each sample point allowing
the calculations behind the Kalman filter to proceed. The
approximation works well when the optimal probability distributions
for state remain close to Gaussian.  If they do not remain
approximately Gaussian, the Extended Kalman Filter can perform poorly,
leading to poor state estimates and impoverished inference.

A second approximation, a grid-based filter, approximates the state
space with a finite grid of points allowing the calculations behind a
hidden (finite) Markov filter to proceed.  A grid-based filter works
well for the lowest dimensional state spaces, but becomes
computationally intractable when the dimensions become even slightly
higher.  In preliminary investigations the electronic warfare problem
that motivated this review [Carver, research paper in preparation],
one and two targets worked well (each adding two dimensions, longitude
and latitude, to the state), whereas three simultaneous targets
remained expensive beyond reach.  In this work, we aimed to find
jammers (``targets'') without bearing information from observing
successful or unsuccessful radio connections to friendlies.

The field calls the last class of filtering approximations that
deserves our attention "particle filters."  In short, the idea
approximates evolving distributions with a finite swarm of Monte Carlo
sample points called particles.  These methods possess great
generality and flexibility, but many researchers find particle methods
more difficult to understand, and to successfully implement, than
their simpler and more straightforward cousins.  Note that there exist
many different ways to implement particle filters, each with its own
benefits and limitations.  We will discuss these methods further in
the next section, as several papers concerning Active Sensing use
particle filters.

\section{Active sensing}

\section{Reinforcement Learning and its variants}

This section spans several disciplines, including reinforement learning, deep 
reinfocement learning, distributional reinforement learning, Bayesian 
reinforcement learning, and multi-agent reinforcement learning.

Let us start by defining the terms above.  Reinforcement learning (RL) extends
machine learning to sequential problems where an agent or agents learn to interact with an 
environment to maximizes cumulative reward.  Deep RL uses neural networks to represent the functions learned by the agent(s).  Classically, RL implementations deal with inevitable uncertainty by representing and maintaining point estimates for all uncertain quantities.  Distributional RL departs from this tradition by maintaining probability distributions for uncertain quantities. If the actor(s) perform inference on these distributions (as they generally do), the actor implements Bayesian RL.

\nocite{*}
\bibliographystyle{ieeetr}
% \bibliographystyle{unsrt}
\bibliography{marl}

\end{document}
